<!DOCTYPE html><html lang="zn-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>pytorch-learn-01 | 闲庭杂记</title><meta name="author" content="Confetti-lxy"><meta name="copyright" content="Confetti-lxy"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="pytorch学习笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch-learn-01">
<meta property="og:url" content="http://blog.confetti-lxy.com/2023/05/12/pytorch-learn-01/index.html">
<meta property="og:site_name" content="闲庭杂记">
<meta property="og:description" content="pytorch学习笔记">
<meta property="og:locale" content="zn_CN">
<meta property="og:image" content="http://blog.confetti-lxy.com/img/cover/cover-01.png">
<meta property="article:published_time" content="2023-05-12T15:25:52.000Z">
<meta property="article:modified_time" content="2023-05-14T06:10:23.907Z">
<meta property="article:author" content="Confetti-lxy">
<meta property="article:tag" content="torch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://blog.confetti-lxy.com/img/cover/cover-01.png"><link rel="shortcut icon" href="/img/common/favicon.ico"><link rel="canonical" href="http://blog.confetti-lxy.com/2023/05/12/pytorch-learn-01/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'pytorch-learn-01',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-05-14 14:10:23'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><script>const preloader = {
  endLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = 'hidden';
    document.getElementById('loading-box').classList.remove("loaded")
  }
}

preloader.initLoading()
window.addEventListener('load',()=> { preloader.endLoading() })

if (false) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/common/avatar.png" onerror="onerror=null;src='/img/common/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">10</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/PL/PL-2.jpeg')"><nav id="nav"><span id="blog-info"><a href="/" title="闲庭杂记"><span class="site-name">闲庭杂记</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">pytorch-learn-01</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-05-12T15:25:52.000Z" title="Created 2023-05-12 23:25:52">2023-05-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-05-14T06:10:23.907Z" title="Updated 2023-05-14 14:10:23">2023-05-14</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/pytorch/">pytorch</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">8.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>51min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="pytorch-learn-01"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="pytorch-workflow-fundamentals">01. PyTorch Workflow
Fundamentals</h1>
<p>The essence of machine learning and deep learning is to take some
data from the past, build an algorithm (like a neural network) to
discover patterns in it and use the discoverd patterns to predict the
future.</p>
<p>There are many ways to do this and many new ways are being discovered
all the time.</p>
<p>But let's start small.</p>
<p>How about we start with a straight line?</p>
<p>And we see if we can build a PyTorch model that learns the pattern of
the straight line and matches it.</p>
<h2 id="what-were-going-to-cover">What we're going to cover</h2>
<p>In this module we're going to cover a standard PyTorch workflow (it
can be chopped and changed as necessary but it covers the main outline
of steps).</p>
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01_a_pytorch_workflow.png" width=900 alt="a pytorch workflow flowchat"/></p>
<p>For now, we'll use this workflow to predict a simple straight line
but the workflow steps can be repeated and changed depending on the
problem you're working on.</p>
<p>Specifically, we're going to cover:</p>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Topic</strong></th>
<th><strong>Contents</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>1. Getting data ready</strong></td>
<td>Data can be almost anything but to get started we're going to create
a simple straight line</td>
</tr>
<tr class="even">
<td><strong>2. Building a model</strong></td>
<td>Here we'll create a model to learn patterns in the data, we'll also
choose a <strong>loss function</strong>, <strong>optimizer</strong> and
build a <strong>training loop</strong>.</td>
</tr>
<tr class="odd">
<td><strong>3. Fitting the model to data (training)</strong></td>
<td>We've got data and a model, now let's let the model (try to) find
patterns in the (<strong>training</strong>) data.</td>
</tr>
<tr class="even">
<td><strong>4. Making predictions and evaluating a model
(inference)</strong></td>
<td>Our model's found patterns in the data, let's compare its findings
to the actual (<strong>testing</strong>) data.</td>
</tr>
<tr class="odd">
<td><strong>5. Saving and loading a model</strong></td>
<td>You may want to use your model elsewhere, or come back to it later,
here we'll cover that.</td>
</tr>
<tr class="even">
<td><strong>6. Putting it all together</strong></td>
<td>Let's take all of the above and combine it.</td>
</tr>
</tbody>
</table>
<h2 id="where-can-can-you-get-help">Where can can you get help?</h2>
<p>All of the materials for this course are <a
target="_blank" rel="noopener" href="https://github.com/mrdbourke/pytorch-deep-learning">available on
GitHub</a>.</p>
<p>And if you run into trouble, you can ask a question on the <a
target="_blank" rel="noopener" href="https://github.com/mrdbourke/pytorch-deep-learning/discussions">Discussions
page</a> there too.</p>
<p>There's also the <a target="_blank" rel="noopener" href="https://discuss.pytorch.org/">PyTorch
developer forums</a>, a very helpful place for all things PyTorch.</p>
<p>Let's start by putting what we're covering into a dictionary to
reference later.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">what_were_covering = &#123;<span class="number">1</span>: <span class="string">&quot;data (prepare and load)&quot;</span>,</span><br><span class="line">    <span class="number">2</span>: <span class="string">&quot;build model&quot;</span>,</span><br><span class="line">    <span class="number">3</span>: <span class="string">&quot;fitting the model to data (training)&quot;</span>,</span><br><span class="line">    <span class="number">4</span>: <span class="string">&quot;making predictions and evaluating a model (inference)&quot;</span>,</span><br><span class="line">    <span class="number">5</span>: <span class="string">&quot;saving and loading a model&quot;</span>,</span><br><span class="line">    <span class="number">6</span>: <span class="string">&quot;putting it all together&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>And now let's import what we'll need for this module.</p>
<p>We're going to get <code>torch</code>, <code>torch.nn</code>
(<code>nn</code> stands for neural network and this package contains the
building blocks for creating neural networks in PyTorch) and
<code>matplotlib</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn <span class="comment"># nn contains all of PyTorch&#x27;s building blocks for neural networks</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check PyTorch version</span></span><br><span class="line">torch.__version__</span><br></pre></td></tr></table></figure>
<pre><code>&#39;1.12.1+cu113&#39;</code></pre>
<h2 id="data-preparing-and-loading">1. Data (preparing and loading)</h2>
<p>I want to stress that "data" in machine learning can be almost
anything you can imagine. A table of numbers (like a big Excel
spreadsheet), images of any kind, videos (YouTube has lots of data!),
audio files like songs or podcasts, protein structures, text and
more.</p>
<figure>
<img
src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01-machine-learning-a-game-of-two-parts.png"
alt="machine learning is a game of two parts: 1. turn your data into a representative set of numbers and 2. build or pick a model to learn the representation as best as possible" />
<figcaption aria-hidden="true">machine learning is a game of two parts:
1. turn your data into a representative set of numbers and 2. build or
pick a model to learn the representation as best as
possible</figcaption>
</figure>
<p>Machine learning is a game of two parts: 1. Turn your data, whatever
it is, into numbers (a representation). 2. Pick or build a model to
learn the representation as best as possible.</p>
<p>Sometimes one and two can be done at the same time.</p>
<p>But what if you don't have data?</p>
<p>Well, that's where we're at now.</p>
<p>No data.</p>
<p>But we can create some.</p>
<p>Let's create our data as a straight line.</p>
<p>We'll use <a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Linear_regression">linear
regression</a> to create the data with known <strong>parameters</strong>
(things that can be learned by a model) and then we'll use PyTorch to
see if we can build model to estimate these parameters using <a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Gradient_descent"><strong>gradient
descent</strong></a>.</p>
<p>Don't worry if the terms above don't mean much now, we'll see them in
action and I'll put extra resources below where you can learn more.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create *known* parameters</span></span><br><span class="line">weight = <span class="number">0.7</span></span><br><span class="line">bias = <span class="number">0.3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data</span></span><br><span class="line">start = <span class="number">0</span></span><br><span class="line">end = <span class="number">1</span></span><br><span class="line">step = <span class="number">0.02</span></span><br><span class="line">X = torch.arange(start, end, step).unsqueeze(dim=<span class="number">1</span>)</span><br><span class="line">y = weight * X + bias</span><br><span class="line"></span><br><span class="line">X[:<span class="number">10</span>], y[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>
<pre><code>(tensor([[0.0000],
         [0.0200],
         [0.0400],
         [0.0600],
         [0.0800],
         [0.1000],
         [0.1200],
         [0.1400],
         [0.1600],
         [0.1800]]),
 tensor([[0.3000],
         [0.3140],
         [0.3280],
         [0.3420],
         [0.3560],
         [0.3700],
         [0.3840],
         [0.3980],
         [0.4120],
         [0.4260]]))</code></pre>
<p>Beautiful! Now we're going to move towards building a model that can
learn the relationship between <code>X</code>
(<strong>features</strong>) and <code>y</code>
(<strong>labels</strong>).</p>
<h3 id="split-data-into-training-and-test-sets">Split data into training
and test sets</h3>
<p>We've got some data.</p>
<p>But before we build a model we need to split it up.</p>
<p>One of most important steps in a machine learning project is creating
a training and test set (and when required, a validation set).</p>
<p>Each split of the dataset serves a specific purpose:</p>
<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th>Split</th>
<th>Purpose</th>
<th>Amount of total data</th>
<th>How often is it used?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Training set</strong></td>
<td>The model learns from this data (like the course materials you study
during the semester).</td>
<td>~60-80%</td>
<td>Always</td>
</tr>
<tr class="even">
<td><strong>Validation set</strong></td>
<td>The model gets tuned on this data (like the practice exam you take
before the final exam).</td>
<td>~10-20%</td>
<td>Often but not always</td>
</tr>
<tr class="odd">
<td><strong>Testing set</strong></td>
<td>The model gets evaluated on this data to test what it has learned
(like the final exam you take at the end of the semester).</td>
<td>~10-20%</td>
<td>Always</td>
</tr>
</tbody>
</table>
<p>For now, we'll just use a training and test set, this means we'll
have a dataset for our model to learn on as well as be evaluated on.</p>
<p>We can create them by splitting our <code>X</code> and <code>y</code>
tensors.</p>
<blockquote>
<p><strong>Note:</strong> When dealing with real-world data, this step
is typically done right at the start of a project (the test set should
always be kept separate from all other data). We want our model to learn
on training data and then evaluate it on test data to get an indication
of how well it <strong>generalizes</strong> to unseen examples.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create train/test split</span></span><br><span class="line">train_split = <span class="built_in">int</span>(<span class="number">0.8</span> * <span class="built_in">len</span>(X)) <span class="comment"># 80% of data used for training set, 20% for testing </span></span><br><span class="line">X_train, y_train = X[:train_split], y[:train_split]</span><br><span class="line">X_test, y_test = X[train_split:], y[train_split:]</span><br><span class="line"></span><br><span class="line"><span class="built_in">len</span>(X_train), <span class="built_in">len</span>(y_train), <span class="built_in">len</span>(X_test), <span class="built_in">len</span>(y_test)</span><br></pre></td></tr></table></figure>
<pre><code>(40, 40, 10, 10)</code></pre>
<p>Wonderful, we've got 40 samples for training (<code>X_train</code>
&amp; <code>y_train</code>) and 10 samples for testing
(<code>X_test</code> &amp; <code>y_test</code>).</p>
<p>The model we create is going to try and learn the relationship
between <code>X_train</code> &amp; <code>y_train</code> and then we will
evaluate what it learns on <code>X_test</code> and
<code>y_test</code>.</p>
<p>But right now our data is just numbers on a page.</p>
<p>Let's create a function to visualize it.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plot_predictions</span>(<span class="params">train_data=X_train, </span></span><br><span class="line"><span class="params">                     train_labels=y_train, </span></span><br><span class="line"><span class="params">                     test_data=X_test, </span></span><br><span class="line"><span class="params">                     test_labels=y_test, </span></span><br><span class="line"><span class="params">                     predictions=<span class="literal">None</span></span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Plots training data, test data and compares predictions.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  plt.figure(figsize=(<span class="number">10</span>, <span class="number">7</span>))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Plot training data in blue</span></span><br><span class="line">  plt.scatter(train_data, train_labels, c=<span class="string">&quot;b&quot;</span>, s=<span class="number">4</span>, label=<span class="string">&quot;Training data&quot;</span>)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Plot test data in green</span></span><br><span class="line">  plt.scatter(test_data, test_labels, c=<span class="string">&quot;g&quot;</span>, s=<span class="number">4</span>, label=<span class="string">&quot;Testing data&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> predictions <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="comment"># Plot the predictions in red (predictions were made on the test data)</span></span><br><span class="line">    plt.scatter(test_data, predictions, c=<span class="string">&quot;r&quot;</span>, s=<span class="number">4</span>, label=<span class="string">&quot;Predictions&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Show the legend</span></span><br><span class="line">  plt.legend(prop=&#123;<span class="string">&quot;size&quot;</span>: <span class="number">14</span>&#125;);</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_predictions();</span><br></pre></td></tr></table></figure>
<p>​<br />
<img src="/image/pytorch-learn-01/output_13_0.png" alt="png" /> ​</p>
<p>Epic!</p>
<p>Now instead of just being numbers on a page, our data is a straight
line.</p>
<blockquote>
<p><strong>Note:</strong> Now's a good time to introduce you to the data
explorer's motto... "visualize, visualize, visualize!"</p>
<p>Think of this whenever you're working with data and turning it into
numbers, if you can visualize something, it can do wonders for
understanding.</p>
<p>Machines love numbers and we humans like numbers too but we also like
to look at things.</p>
</blockquote>
<h2 id="build-model">2. Build model</h2>
<p>Now we've got some data, let's build a model to use the blue dots to
predict the green dots.</p>
<p>We're going to jump right in.</p>
<p>We'll write the code first and then explain everything.</p>
<p>Let's replicate a standard linear regression model using pure
PyTorch.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a Linear Regression model class</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearRegressionModel</span>(nn.Module): <span class="comment"># &lt;- almost everything in PyTorch is a nn.Module (think of this as neural network lego blocks)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__() </span><br><span class="line">        self.weights = nn.Parameter(torch.randn(<span class="number">1</span>, <span class="comment"># &lt;- start with random weights (this will get adjusted as the model learns)</span></span><br><span class="line">                                                dtype=torch.<span class="built_in">float</span>), <span class="comment"># &lt;- PyTorch loves float32 by default</span></span><br><span class="line">                                   requires_grad=<span class="literal">True</span>) <span class="comment"># &lt;- can we update this value with gradient descent?)</span></span><br><span class="line"></span><br><span class="line">        self.bias = nn.Parameter(torch.randn(<span class="number">1</span>, <span class="comment"># &lt;- start with random bias (this will get adjusted as the model learns)</span></span><br><span class="line">                                            dtype=torch.<span class="built_in">float</span>), <span class="comment"># &lt;- PyTorch loves float32 by default</span></span><br><span class="line">                                requires_grad=<span class="literal">True</span>) <span class="comment"># &lt;- can we update this value with gradient descent?))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Forward defines the computation in the model</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor: <span class="comment"># &lt;- &quot;x&quot; is the input data (e.g. training/testing features)</span></span><br><span class="line">        <span class="keyword">return</span> self.weights * x + self.bias <span class="comment"># &lt;- this is the linear regression formula (y = m*x + b)</span></span><br></pre></td></tr></table></figure>
<p>Alright there's a fair bit going on above but let's break it down bit
by bit.</p>
<blockquote>
<p><strong>Resource:</strong> We'll be using Python classes to create
bits and pieces for building neural networks. If you're unfamiliar with
Python class notation, I'd recommend reading <a
target="_blank" rel="noopener" href="https://realpython.com/python3-object-oriented-programming/">Real
Python's Object Orientating programming in Python 3 guide</a> a few
times.</p>
</blockquote>
<h3 id="pytorch-model-building-essentials">PyTorch model building
essentials</h3>
<p>PyTorch has four (give or take) essential modules you can use to
create almost any kind of neural network you can imagine.</p>
<p>They are <a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html"><code>torch.nn</code></a>,
<a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html"><code>torch.optim</code></a>,
<a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset"><code>torch.utils.data.Dataset</code></a>
and <a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/data.html"><code>torch.utils.data.DataLoader</code></a>.
For now, we'll focus on the first two and get to the other two later
(though you may be able to guess what they do).</p>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="header">
<th>PyTorch module</th>
<th>What does it do?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html"><code>torch.nn</code></a></td>
<td>Contains all of the building blocks for computational graphs
(essentially a series of computations executed in a particular
way).</td>
</tr>
<tr class="even">
<td><a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#parameter"><code>torch.nn.Parameter</code></a></td>
<td>Stores tensors that can be used with <code>nn.Module</code>. If
<code>requires_grad=True</code> gradients (used for updating model
parameters via <a
target="_blank" rel="noopener" href="https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html"><strong>gradient
descent</strong></a>) are calculated automatically, this is often
referred to as "autograd".</td>
</tr>
<tr class="odd">
<td><a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module"><code>torch.nn.Module</code></a></td>
<td>The base class for all neural network modules, all the building
blocks for neural networks are subclasses. If you're building a neural
network in PyTorch, your models should subclass <code>nn.Module</code>.
Requires a <code>forward()</code> method be implemented.</td>
</tr>
<tr class="even">
<td><a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html"><code>torch.optim</code></a></td>
<td>Contains various optimization algorithms (these tell the model
parameters stored in <code>nn.Parameter</code> how to best change to
improve gradient descent and in turn reduce the loss).</td>
</tr>
<tr class="odd">
<td><code>def forward()</code></td>
<td>All <code>nn.Module</code> subclasses require a
<code>forward()</code> method, this defines the computation that will
take place on the data passed to the particular <code>nn.Module</code>
(e.g. the linear regression formula above).</td>
</tr>
</tbody>
</table>
<p>If the above sounds complex, think of like this, almost everything in
a PyTorch neural network comes from <code>torch.nn</code>, *
<code>nn.Module</code> contains the larger building blocks (layers) *
<code>nn.Parameter</code> contains the smaller parameters like weights
and biases (put these together to make <code>nn.Module</code>(s)) *
<code>forward()</code> tells the larger blocks how to make calculations
on inputs (tensors full of data) within <code>nn.Module</code>(s) *
<code>torch.optim</code> contains optimization methods on how to improve
the parameters within <code>nn.Parameter</code> to better represent
input data</p>
<p><img
src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01-pytorch-linear-model-annotated.png"
alt="a pytorch linear model with annotations" /> <em>Basic building
blocks of creating a PyTorch model by subclassing
<code>nn.Module</code>. For objects that subclass
<code>nn.Module</code>, the <code>forward()</code> method must be
defined.</em></p>
<blockquote>
<p><strong>Resource:</strong> See more of these essential modules and
their uses cases in the <a
target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/ptcheat.html">PyTorch Cheat
Sheet</a>.</p>
</blockquote>
<h3 id="checking-the-contents-of-a-pytorch-model">Checking the contents
of a PyTorch model</h3>
<p>Now we've got these out of the way, let's create a model instance
with the class we've made and check its parameters using <a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters"><code>.parameters()</code></a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set manual seed since nn.Parameter are randomly initialzied</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create an instance of the model (this is a subclass of nn.Module that contains nn.Parameter(s))</span></span><br><span class="line">model_0 = LinearRegressionModel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check the nn.Parameter(s) within the nn.Module subclass we created</span></span><br><span class="line"><span class="built_in">list</span>(model_0.parameters())</span><br></pre></td></tr></table></figure>
<pre><code>[Parameter containing:
 tensor([0.3367], requires_grad=True),
 Parameter containing:
 tensor([0.1288], requires_grad=True)]</code></pre>
<p>We can also get the state (what the model contains) of the model
using <a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict"><code>.state_dict()</code></a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># List named parameters </span></span><br><span class="line">model_0.state_dict()</span><br></pre></td></tr></table></figure>
<pre><code>OrderedDict([(&#39;weights&#39;, tensor([0.3367])), (&#39;bias&#39;, tensor([0.1288]))])</code></pre>
<p>Notice how the values for <code>weights</code> and <code>bias</code>
from <code>model_0.state_dict()</code> come out as random float
tensors?</p>
<p>This is becuase we initialized them above using
<code>torch.randn()</code>.</p>
<p>Essentially we want to start from random parameters and get the model
to update them towards parameters that fit our data best (the hardcoded
<code>weight</code> and <code>bias</code> values we set when creating
our straight line data).</p>
<blockquote>
<p><strong>Exercise:</strong> Try changing the
<code>torch.manual_seed()</code> value two cells above, see what happens
to the weights and bias values.</p>
</blockquote>
<p>Because our model starts with random values, right now it'll have
poor predictive power.</p>
<h3 id="making-predictions-using-torch.inference_mode">Making
predictions using <code>torch.inference_mode()</code></h3>
<p>To check this we can pass it the test data <code>X_test</code> to see
how closely it predicts <code>y_test</code>.</p>
<p>When we pass data to our model, it'll go through the model's
<code>forward()</code> method and produce a result using the computation
we've defined.</p>
<p>Let's make some predictions.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Make predictions with model</span></span><br><span class="line"><span class="keyword">with</span> torch.inference_mode(): </span><br><span class="line">    y_preds = model_0(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Note: in older PyTorch code you might also see torch.no_grad()</span></span><br><span class="line"><span class="comment"># with torch.no_grad():</span></span><br><span class="line"><span class="comment">#   y_preds = model_0(X_test)</span></span><br></pre></td></tr></table></figure>
<p>Hmm?</p>
<p>You probably noticed we used <a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.inference_mode.html"><code>torch.inference_mode()</code></a>
as a <a target="_blank" rel="noopener" href="https://realpython.com/python-with-statement/">context
manager</a> (that's what the <code>with torch.inference_mode():</code>
is) to make the predictions.</p>
<p>As the name suggests, <code>torch.inference_mode()</code> is used
when using a model for inference (making predictions).</p>
<p><code>torch.inference_mode()</code> turns off a bunch of things (like
gradient tracking, which is necessary for training but not for
inference) to make <strong>forward-passes</strong> (data going through
the <code>forward()</code> method) faster.</p>
<blockquote>
<p><strong>Note:</strong> In older PyTorch code, you may also see
<code>torch.no_grad()</code> being used for inference. While
<code>torch.inference_mode()</code> and <code>torch.no_grad()</code> do
similar things, <code>torch.inference_mode()</code> is newer,
potentially faster and preferred. See this <a
target="_blank" rel="noopener" href="https://twitter.com/PyTorch/status/1437838231505096708?s=20">Tweet
from PyTorch</a> for more.</p>
</blockquote>
<p>We've made some predictions, let's see what they look like.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Check the predictions</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Number of testing samples: <span class="subst">&#123;<span class="built_in">len</span>(X_test)&#125;</span>&quot;</span>) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Number of predictions made: <span class="subst">&#123;<span class="built_in">len</span>(y_preds)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Predicted values:\n<span class="subst">&#123;y_preds&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Number of testing samples: 10
Number of predictions made: 10
Predicted values:
tensor([[0.3982],
        [0.4049],
        [0.4116],
        [0.4184],
        [0.4251],
        [0.4318],
        [0.4386],
        [0.4453],
        [0.4520],
        [0.4588]])</code></pre>
<p>Notice how there's one prediction value per testing sample.</p>
<p>This is because of the kind of data we're using. For our straight
line, one <code>X</code> value maps to one <code>y</code> value.</p>
<p>However, machine learning models are very flexible. You could have
100 <code>X</code> values mapping to one, two, three or 10
<code>y</code> values. It all depends on what you're working on.</p>
<p>Our predictions are still numbers on a page, let's visualize them
with our <code>plot_predictions()</code> function we created above.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_predictions(predictions=y_preds)</span><br></pre></td></tr></table></figure>
<p>​<br />
<img src="/image/pytorch-learn-01/output_29_0.png" alt="png" /> ​</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_test - y_preds</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[0.4618],
        [0.4691],
        [0.4764],
        [0.4836],
        [0.4909],
        [0.4982],
        [0.5054],
        [0.5127],
        [0.5200],
        [0.5272]])</code></pre>
<p>Woah! Those predictions look pretty bad...</p>
<p>This make sense though when you remember our model is just using
random parameter values to make predictions.</p>
<p>It hasn't even looked at the blue dots to try to predict the green
dots.</p>
<p>Time to change that.</p>
<h2 id="train-model">3. Train model</h2>
<p>Right now our model is making predictions using random parameters to
make calculations, it's basically guessing (randomly).</p>
<p>To fix that, we can update its internal parameters (I also refer to
<em>parameters</em> as patterns), the <code>weights</code> and
<code>bias</code> values we set randomly using
<code>nn.Parameter()</code> and <code>torch.randn()</code> to be
something that better represents the data.</p>
<p>We could hard code this (since we know the default values
<code>weight=0.7</code> and <code>bias=0.3</code>) but where's the fun
in that?</p>
<p>Much of the time you won't know what the ideal parameters are for a
model.</p>
<p>Instead, it's much more fun to write code to see if the model can try
and figure them out itself.</p>
<h3 id="creating-a-loss-function-and-optimizer-in-pytorch">Creating a
loss function and optimizer in PyTorch</h3>
<p>For our model to update its parameters on its own, we'll need to add
a few more things to our recipe.</p>
<p>And that's a <strong>loss function</strong> as well as an
<strong>optimizer</strong>.</p>
<p>The rolls of these are:</p>
<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th>Function</th>
<th>What does it do?</th>
<th>Where does it live in PyTorch?</th>
<th>Common values</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Loss function</strong></td>
<td>Measures how wrong your models predictions (e.g.
<code>y_preds</code>) are compared to the truth labels (e.g.
<code>y_test</code>). Lower the better.</td>
<td>PyTorch has plenty of built-in loss functions in <a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#loss-functions"><code>torch.nn</code></a>.</td>
<td>Mean absolute error (MAE) for regression problems (<a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html"><code>torch.nn.L1Loss()</code></a>).
Binary cross entropy for binary classification problems (<a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html"><code>torch.nn.BCELoss()</code></a>).</td>
</tr>
<tr class="even">
<td><strong>Optimizer</strong></td>
<td>Tells your model how to update its internal parameters to best lower
the loss.</td>
<td>You can find various optimization function implementations in <a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html"><code>torch.optim</code></a>.</td>
<td>Stochastic gradient descent (<a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD"><code>torch.optim.SGD()</code></a>).
Adam optimizer (<a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam"><code>torch.optim.Adam()</code></a>).</td>
</tr>
</tbody>
</table>
<p>Let's create a loss function and an optimizer we can use to help
improve our model.</p>
<p>Depending on what kind of problem you're working on will depend on
what loss function and what optimizer you use.</p>
<p>However, there are some common values, that are known to work well
such as the SGD (stochastic gradient descent) or Adam optimizer. And the
MAE (mean absolute error) loss function for regression problems
(predicting a number) or binary cross entropy loss function for
classification problems (predicting one thing or another).</p>
<p>For our problem, since we're predicting a number, let's use MAE
(which is under <code>torch.nn.L1Loss()</code>) in PyTorch as our loss
function.</p>
<p><img
src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01-mae-loss-annotated.png"
alt="what MAE loss looks like for our plot data" /> <em>Mean absolute
error (MAE, in PyTorch: <code>torch.nn.L1Loss</code>) measures the
absolute difference between two points (predictions and labels) and then
takes the mean across all examples.</em></p>
<p>And we'll use SGD, <code>torch.optim.SGD(params, lr)</code>
where:</p>
<ul>
<li><code>params</code> is the target model parameters you'd like to
optimize (e.g. the <code>weights</code> and <code>bias</code> values we
randomly set before).</li>
<li><code>lr</code> is the <strong>learning rate</strong> you'd like the
optimizer to update the parameters at, higher means the optimizer will
try larger updates (these can sometimes be too large and the optimizer
will fail to work), lower means the optimizer will try smaller updates
(these can sometimes be too small and the optimizer will take too long
to find the ideal values). The learning rate is considered a
<strong>hyperparameter</strong> (because it's set by a machine learning
engineer). Common starting values for the learning rate are
<code>0.01</code>, <code>0.001</code>, <code>0.0001</code>, however,
these can also be adjusted over time (this is called <a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate">learning
rate scheduling</a>).</li>
</ul>
<p>Woah, that's a lot, let's see it in code.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create the loss function</span></span><br><span class="line">loss_fn = nn.L1Loss() <span class="comment"># MAE loss is same as L1Loss</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the optimizer</span></span><br><span class="line">optimizer = torch.optim.SGD(params=model_0.parameters(), <span class="comment"># parameters of target model to optimize</span></span><br><span class="line">                            lr=<span class="number">0.01</span>) <span class="comment"># learning rate (how much the optimizer should change parameters at each step, higher=more (less stable), lower=less (might take a long time))</span></span><br></pre></td></tr></table></figure>
<h3 id="creating-an-optimization-loop-in-pytorch">Creating an
optimization loop in PyTorch</h3>
<p>Woohoo! Now we've got a loss function and an optimizer, it's now time
to create a <strong>training loop</strong> (and <strong>testing
loop</strong>).</p>
<p>The training loop involves the model going through the training data
and learning the relationships between the <code>features</code> and
<code>labels</code>.</p>
<p>The testing loop involves going through the testing data and
evaluating how good the patterns are that the model learned on the
training data (the model never see's the testing data during
training).</p>
<p>Each of these is called a "loop" because we want our model to look
(loop through) at each sample in each dataset.</p>
<p>To create these we're going to write a Python <code>for</code> loop
in the theme of the <a
target="_blank" rel="noopener" href="https://twitter.com/mrdbourke/status/1450977868406673410?s=20">unofficial
PyTorch optimization loop song</a> (there's a <a
target="_blank" rel="noopener" href="https://youtu.be/Nutpusq_AFw">video version too</a>).</p>
<p><img
src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01-pytorch-optimization-loop-song.png"
alt="the unofficial pytorch optimization loop song" /> <em>The unoffical
PyTorch optimization loops song, a fun way to remember the steps in a
PyTorch training (and testing) loop.</em></p>
<p>There will be a fair bit of code but nothing we can't handle.</p>
<h3 id="pytorch-training-loop">PyTorch training loop</h3>
<p>For the training loop, we'll build the following steps:</p>
<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th>Number</th>
<th>Step name</th>
<th>What does it do?</th>
<th>Code example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Forward pass</td>
<td>The model goes through all of the training data once, performing its
<code>forward()</code> function calculations.</td>
<td><code>model(x_train)</code></td>
</tr>
<tr class="even">
<td>2</td>
<td>Calculate the loss</td>
<td>The model's outputs (predictions) are compared to the ground truth
and evaluated to see how wrong they are.</td>
<td><code>loss = loss_fn(y_pred, y_train)</code></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Zero gradients</td>
<td>The optimizers gradients are set to zero (they are accumulated by
default) so they can be recalculated for the specific training
step.</td>
<td><code>optimizer.zero_grad()</code></td>
</tr>
<tr class="even">
<td>4</td>
<td>Perform backpropagation on the loss</td>
<td>Computes the gradient of the loss with respect for every model
parameter to be updated (each parameter with
<code>requires_grad=True</code>). This is known as
<strong>backpropagation</strong>, hence "backwards".</td>
<td><code>loss.backward()</code></td>
</tr>
<tr class="odd">
<td>5</td>
<td>Update the optimizer (<strong>gradient descent</strong>)</td>
<td>Update the parameters with <code>requires_grad=True</code> with
respect to the loss gradients in order to improve them.</td>
<td><code>optimizer.step()</code></td>
</tr>
</tbody>
</table>
<figure>
<img
src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01-pytorch-training-loop-annotated.png"
alt="pytorch training loop annotated" />
<figcaption aria-hidden="true">pytorch training loop
annotated</figcaption>
</figure>
<blockquote>
<p><strong>Note:</strong> The above is just one example of how the steps
could be ordered or described. With experience you'll find making
PyTorch training loops can be quite flexible.</p>
<p>And on the ordering of things, the above is a good default order but
you may see slightly different orders. Some rules of thumb: * Calculate
the loss (<code>loss = ...</code>) <em>before</em> performing
backpropagation on it (<code>loss.backward()</code>). * Zero gradients
(<code>optimizer.zero_grad()</code>) <em>before</em> stepping them
(<code>optimizer.step()</code>). * Step the optimizer
(<code>optimizer.step()</code>) <em>after</em> performing
backpropagation on the loss (<code>loss.backward()</code>).</p>
</blockquote>
<p>For resources to help understand what's happening behind the scenes
with backpropagation and gradient descent, see the extra-curriculum
section.</p>
<h3 id="pytorch-testing-loop">PyTorch testing loop</h3>
<p>As for the testing loop (evaluating our model), the typical steps
include:</p>
<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th>Number</th>
<th>Step name</th>
<th>What does it do?</th>
<th>Code example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Forward pass</td>
<td>The model goes through all of the training data once, performing its
<code>forward()</code> function calculations.</td>
<td><code>model(x_test)</code></td>
</tr>
<tr class="even">
<td>2</td>
<td>Calculate the loss</td>
<td>The model's outputs (predictions) are compared to the ground truth
and evaluated to see how wrong they are.</td>
<td><code>loss = loss_fn(y_pred, y_test)</code></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Calulate evaluation metrics (optional)</td>
<td>Alongisde the loss value you may want to calculate other evaluation
metrics such as accuracy on the test set.</td>
<td>Custom functions</td>
</tr>
</tbody>
</table>
<p>Notice the testing loop doesn't contain performing backpropagation
(<code>loss.backward()</code>) or stepping the optimizer
(<code>optimizer.step()</code>), this is because no parameters in the
model are being changed during testing, they've already been calculated.
For testing, we're only interested in the output of the forward pass
through the model.</p>
<figure>
<img
src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01-pytorch-testing-loop-annotated.png"
alt="pytorch annotated testing loop" />
<figcaption aria-hidden="true">pytorch annotated testing
loop</figcaption>
</figure>
<p>Let's put all of the above together and train our model for 100
<strong>epochs</strong> (forward passes through the data) and we'll
evaluate it every 10 epochs.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the number of epochs (how many times the model will pass over the training data)</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create empty loss lists to track values</span></span><br><span class="line">train_loss_values = []</span><br><span class="line">test_loss_values = []</span><br><span class="line">epoch_count = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment">### Training</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Put model in training mode (this is the default state of a model)</span></span><br><span class="line">    model_0.train()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. Forward pass on train data using the forward() method inside </span></span><br><span class="line">    y_pred = model_0(X_train)</span><br><span class="line">    <span class="comment"># print(y_pred)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. Calculate the loss (how different are our models predictions to the ground truth)</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. Zero grad of the optimizer</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. Loss backwards</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. Progress the optimizer</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment">### Testing</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Put the model in evaluation mode</span></span><br><span class="line">    model_0.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">      <span class="comment"># 1. Forward pass on test data</span></span><br><span class="line">      test_pred = model_0(X_test)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># 2. Caculate loss on test data</span></span><br><span class="line">      test_loss = loss_fn(test_pred, y_test.<span class="built_in">type</span>(torch.<span class="built_in">float</span>)) <span class="comment"># predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type</span></span><br><span class="line"></span><br><span class="line">      <span class="comment"># Print out what&#x27;s happening</span></span><br><span class="line">      <span class="keyword">if</span> epoch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            epoch_count.append(epoch)</span><br><span class="line">            train_loss_values.append(loss.detach().numpy())</span><br><span class="line">            test_loss_values.append(test_loss.detach().numpy())</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch&#125;</span> | MAE Train Loss: <span class="subst">&#123;loss&#125;</span> | MAE Test Loss: <span class="subst">&#123;test_loss&#125;</span> &quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch: 0 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.48106518387794495 
Epoch: 10 | MAE Train Loss: 0.1976713240146637 | MAE Test Loss: 0.3463551998138428 
Epoch: 20 | MAE Train Loss: 0.08908725529909134 | MAE Test Loss: 0.21729660034179688 
Epoch: 30 | MAE Train Loss: 0.053148526698350906 | MAE Test Loss: 0.14464017748832703 
Epoch: 40 | MAE Train Loss: 0.04543796554207802 | MAE Test Loss: 0.11360953003168106 
Epoch: 50 | MAE Train Loss: 0.04167863354086876 | MAE Test Loss: 0.09919948130846024 
Epoch: 60 | MAE Train Loss: 0.03818932920694351 | MAE Test Loss: 0.08886633068323135 
Epoch: 70 | MAE Train Loss: 0.03476089984178543 | MAE Test Loss: 0.0805937647819519 
Epoch: 80 | MAE Train Loss: 0.03132382780313492 | MAE Test Loss: 0.07232122868299484 
Epoch: 90 | MAE Train Loss: 0.02788739837706089 | MAE Test Loss: 0.06473556160926819 </code></pre>
<p>Oh would you look at that! Looks like our loss is going down with
every epoch, let's plot it to find out.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot the loss curves</span></span><br><span class="line">plt.plot(epoch_count, train_loss_values, label=<span class="string">&quot;Train loss&quot;</span>)</span><br><span class="line">plt.plot(epoch_count, test_loss_values, label=<span class="string">&quot;Test loss&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Training and test loss curves&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Loss&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Epochs&quot;</span>)</span><br><span class="line">plt.legend();</span><br></pre></td></tr></table></figure>
<p>​<br />
<img src="/image/pytorch-learn-01/output_40_0.png" alt="png" /> ​</p>
<p>Nice! The <strong>loss curves</strong> show the loss going down over
time. Remember, loss is the measure of how <em>wrong</em> your model is,
so the lower the better.</p>
<p>But why did the loss go down?</p>
<p>Well, thanks to our loss function and optimizer, the model's internal
parameters (<code>weights</code> and <code>bias</code>) were updated to
better reflect the underlying patterns in the data.</p>
<p>Let's inspect our model's <a
target="_blank" rel="noopener" href="https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html"><code>.state_dict()</code></a>
to see see how close our model gets to the original values we set for
weights and bias.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Find our model&#x27;s learned parameters</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The model learned the following values for weights and bias:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model_0.state_dict())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nAnd the original values for weights and bias are:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;weights: <span class="subst">&#123;weight&#125;</span>, bias: <span class="subst">&#123;bias&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>The model learned the following values for weights and bias:
OrderedDict([(&#39;weights&#39;, tensor([0.5784])), (&#39;bias&#39;, tensor([0.3513]))])

And the original values for weights and bias are:
weights: 0.7, bias: 0.3</code></pre>
<p>Wow! How cool is that?</p>
<p>Our model got very close to calculate the exact original values for
<code>weight</code> and <code>bias</code> (and it would probably get
even closer if we trained it for longer).</p>
<blockquote>
<p><strong>Exercise:</strong> Try changing the <code>epochs</code> value
above to 200, what happens to the loss curves and the weights and bias
parameter values of the model?</p>
</blockquote>
<p>It'd likely never guess them <em>perfectly</em> (especially when
using more complicated datasets) but that's okay, often you can do very
cool things with a close approximation.</p>
<p>This is the whole idea of machine learning and deep learning,
<strong>there are some ideal values that describe our data</strong> and
rather than figuring them out by hand, <strong>we can train a model to
figure them out programmatically</strong>.</p>
<h2 id="making-predictions-with-a-trained-pytorch-model-inference">4.
Making predictions with a trained PyTorch model (inference)</h2>
<p>Once you've trained a model, you'll likely want to make predictions
with it.</p>
<p>We've already seen a glimpse of this in the training and testing code
above, the steps to do it outside of the training/testing loop are
similar.</p>
<p>There are three things to remember when making predictions (also
called performing inference) with a PyTorch model:</p>
<ol type="1">
<li>Set the model in evaluation mode (<code>model.eval()</code>).</li>
<li>Make the predictions using the inference mode context manager
(<code>with torch.inference_mode(): ...</code>).</li>
<li>All predictions should be made with objects on the same device (e.g.
data and model on GPU only or data and model on CPU only).</li>
</ol>
<p>The first two items make sure all helpful calculations and settings
PyTorch uses behind the scenes during training but aren't necessary for
inference are turned off (this results in faster computation). And the
third ensures that you won't run into cross-device errors.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Set the model in evaluation mode</span></span><br><span class="line">model_0.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Setup the inference mode context manager</span></span><br><span class="line"><span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">  <span class="comment"># 3. Make sure the calculations are done with the model and data on the same device</span></span><br><span class="line">  <span class="comment"># in our case, we haven&#x27;t setup device-agnostic code yet so our data and model are</span></span><br><span class="line">  <span class="comment"># on the CPU by default.</span></span><br><span class="line">  <span class="comment"># model_0.to(device)</span></span><br><span class="line">  <span class="comment"># X_test = X_test.to(device)</span></span><br><span class="line">  y_preds = model_0(X_test)</span><br><span class="line">y_preds</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[0.8141],
        [0.8256],
        [0.8372],
        [0.8488],
        [0.8603],
        [0.8719],
        [0.8835],
        [0.8950],
        [0.9066],
        [0.9182]])</code></pre>
<p>Nice! We've made some predictions with our trained model, now how do
they look?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_predictions(predictions=y_preds)</span><br></pre></td></tr></table></figure>
<p>​<br />
<img src="/image/pytorch-learn-01/output_47_0.png" alt="png" /> ​</p>
<p>Woohoo! Those red dots are looking far closer than they were
before!</p>
<p>Let's get onto saving an reloading a model in PyTorch.</p>
<h2 id="saving-and-loading-a-pytorch-model">5. Saving and loading a
PyTorch model</h2>
<p>If you've trained a PyTorch model, chances are you'll want to save it
and export it somewhere.</p>
<p>As in, you might train it on Google Colab or your local machine with
a GPU but you'd like to now export it to some sort of application where
others can use it.</p>
<p>Or maybe you'd like to save your progress on a model and come back
and load it back later.</p>
<p>For saving and loading models in PyTorch, there are three main
methods you should be aware of (all of below have been taken from the <a
target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference">PyTorch
saving and loading models guide</a>):</p>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="header">
<th>PyTorch method</th>
<th>What does it do?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html?highlight=save#torch.save"><code>torch.save</code></a></td>
<td>Saves a serialzed object to disk using Python's <a
target="_blank" rel="noopener" href="https://docs.python.org/3/library/pickle.html"><code>pickle</code></a>
utility. Models, tensors and various other Python objects like
dictionaries can be saved using <code>torch.save</code>.</td>
</tr>
<tr class="even">
<td><a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html?highlight=torch%20load#torch.load"><code>torch.load</code></a></td>
<td>Uses <code>pickle</code>'s unpickling features to deserialize and
load pickled Python object files (like models, tensors or dictionaries)
into memory. You can also set which device to load the object to (CPU,
GPU etc).</td>
</tr>
<tr class="odd">
<td><a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=load_state_dict#torch.nn.Module.load_state_dict"><code>torch.nn.Module.load_state_dict</code></a></td>
<td>Loads a model's parameter dictionary
(<code>model.state_dict()</code>) using a saved
<code>state_dict()</code> object.</td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>Note:</strong> As stated in <a
target="_blank" rel="noopener" href="https://docs.python.org/3/library/pickle.html">Python's
<code>pickle</code> documentation</a>, the <code>pickle</code> module
<strong>is not secure</strong>. That means you should only ever unpickle
(load) data you trust. That goes for loading PyTorch models as well.
Only ever use saved PyTorch models from sources you trust.</p>
</blockquote>
<h3 id="saving-a-pytorch-models-state_dict">Saving a PyTorch model's
<code>state_dict()</code></h3>
<p>The <a
target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference">recommended
way</a> for saving and loading a model for inference (making
predictions) is by saving and loading a model's
<code>state_dict()</code>.</p>
<p>Let's see how we can do that in a few steps:</p>
<ol type="1">
<li>We'll create a directory for saving models to called
<code>models</code> using Python's <code>pathlib</code> module.</li>
<li>We'll create a file path to save the model to.</li>
<li>We'll call <code>torch.save(obj, f)</code> where <code>obj</code> is
the target model's <code>state_dict()</code> and <code>f</code> is the
filename of where to save the model.</li>
</ol>
<blockquote>
<p><strong>Note:</strong> It's common convention for PyTorch saved
models or objects to end with <code>.pt</code> or <code>.pth</code>,
like <code>saved_model_01.pth</code>.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Create models directory </span></span><br><span class="line">MODEL_PATH = Path(<span class="string">&quot;models&quot;</span>)</span><br><span class="line">MODEL_PATH.mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Create model save path </span></span><br><span class="line">MODEL_NAME = <span class="string">&quot;01_pytorch_workflow_model_0.pth&quot;</span></span><br><span class="line">MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Save the model state dict </span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Saving model to: <span class="subst">&#123;MODEL_SAVE_PATH&#125;</span>&quot;</span>)</span><br><span class="line">torch.save(obj=model_0.state_dict(), <span class="comment"># only saving the state_dict() only saves the models learned parameters</span></span><br><span class="line">           f=MODEL_SAVE_PATH) </span><br></pre></td></tr></table></figure>
<pre><code>Saving model to: models/01_pytorch_workflow_model_0.pth</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Check the saved file path</span></span><br><span class="line">!ls -l models/01_pytorch_workflow_model_0.pth</span><br></pre></td></tr></table></figure>
<pre><code>-rw-rw-r-- 1 daniel daniel 1063 Nov 10 16:07 models/01_pytorch_workflow_model_0.pth</code></pre>
<h3 id="loading-a-saved-pytorch-models-state_dict">Loading a saved
PyTorch model's <code>state_dict()</code></h3>
<p>Since we've now got a saved model <code>state_dict()</code> at
<code>models/01_pytorch_workflow_model_0.pth</code> we can now load it
in using <code>torch.nn.Module.load_state_dict(torch.load(f))</code>
where <code>f</code> is the filepath of our saved model
<code>state_dict()</code>.</p>
<p>Why call <code>torch.load()</code> inside
<code>torch.nn.Module.load_state_dict()</code>?</p>
<p>Because we only saved the model's <code>state_dict()</code> which is
a dictionary of learned parameters and not the <em>entire</em> model, we
first have to load the <code>state_dict()</code> with
<code>torch.load()</code> and then pass that <code>state_dict()</code>
to a new instance of our model (which is a subclass of
<code>nn.Module</code>).</p>
<p>Why not save the entire model?</p>
<p><a
target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/saving_loading_models.html#save-load-entire-model">Saving
the entire model</a> rather than just the <code>state_dict()</code> is
more intuitive, however, to quote the PyTorch documentation (italics
mine):</p>
<blockquote>
<p>The disadvantage of this approach <em>(saving the whole model)</em>
is that the serialized data is bound to the specific classes and the
exact directory structure used when the model is saved...</p>
<p>Because of this, your code can break in various ways when used in
other projects or after refactors.</p>
</blockquote>
<p>So instead, we're using the flexible method of saving and loading
just the <code>state_dict()</code>, which again is basically a
dictionary of model parameters.</p>
<p>Let's test it out by created another instance of
<code>LinearRegressionModel()</code>, which is a subclass of
<code>torch.nn.Module</code> and will hence have the in-built method
<code>load_state_dit()</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Instantiate a new instance of our model (this will be instantiated with random weights)</span></span><br><span class="line">loaded_model_0 = LinearRegressionModel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the state_dict of our saved model (this will update the new instance of our model with trained weights)</span></span><br><span class="line">loaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))</span><br></pre></td></tr></table></figure>
<pre><code>&lt;All keys matched successfully&gt;</code></pre>
<p>Excellent! It looks like things matched up.</p>
<p>Now to test our loaded model, let's perform inference with it (make
predictions) on the test data.</p>
<p>Remember the rules for performing inference with PyTorch models?</p>
<p>If not, here's a refresher:</p>
<details>
<summary>
PyTorch inference rules
</summary>
<ol>
<li>
Set the model in evaluation mode (<code>model.eval()</code>).
</li>
<li>
Make the predictions using the inference mode context manager
(<code>with torch.inference_mode(): ...</code>).
</li>
<li>
All predictions should be made with objects on the same device (e.g.
data and model on GPU only or data and model on CPU only).
</li>
</ol>
</details>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Put the loaded model into evaluation mode</span></span><br><span class="line">loaded_model_0.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Use the inference mode context manager to make predictions</span></span><br><span class="line"><span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">    loaded_model_preds = loaded_model_0(X_test) <span class="comment"># perform a forward pass on the test data with the loaded model</span></span><br></pre></td></tr></table></figure>
<p>Now we've made some predictions with the loaded model, let's see if
they're the same as the previous predictions.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Compare previous model predictions with loaded model predictions (these should be the same)</span></span><br><span class="line">y_preds == loaded_model_preds</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[True],
        [True],
        [True],
        [True],
        [True],
        [True],
        [True],
        [True],
        [True],
        [True]])</code></pre>
<p>Nice!</p>
<p>It looks like the loaded model predictions are the same as the
previous model predictions (predictions made prior to saving). This
indicates our model is saving and loading as expected.</p>
<blockquote>
<p><strong>Note:</strong> There are more methods to save and load
PyTorch models but I'll leave these for extra-curriculum and further
reading. See the <a
target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-and-loading-models">PyTorch
guide for saving and loading models</a> for more.</p>
</blockquote>
<h2 id="putting-it-all-together">6. Putting it all together</h2>
<p>We've covered a fair bit of ground so far.</p>
<p>But once you've had some practice, you'll be performing the above
steps like dancing down the street.</p>
<p>Speaking of practice, let's put everything we've done so far
together.</p>
<p>Except this time we'll make our code device agnostic (so if there's a
GPU available, it'll use it and if not, it will default to the CPU).</p>
<p>There'll be far less commentary in this section than above since what
we're going to go through has already been covered.</p>
<p>We'll start by importing the standard libraries we need.</p>
<blockquote>
<p><strong>Note:</strong> If you're using Google Colab, to setup a GPU,
go to Runtime -&gt; Change runtime type -&gt; Hardware acceleration
-&gt; GPU. If you do this, it will reset the Colab runtime and you will
lose saved variables.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import PyTorch and matplotlib</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn <span class="comment"># nn contains all of PyTorch&#x27;s building blocks for neural networks</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check PyTorch version</span></span><br><span class="line">torch.__version__</span><br></pre></td></tr></table></figure>
<pre><code>&#39;1.12.1+cu113&#39;</code></pre>
<p>Now let's start making our code device agnostic by setting
<code>device="cuda"</code> if it's available, otherwise it'll default to
<code>device="cpu"</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Setup device agnostic code</span></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Using device: <span class="subst">&#123;device&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Using device: cuda</code></pre>
<p>If you've got access to a GPU, the above should've printed out:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Using device: cuda</span><br></pre></td></tr></table></figure>
<p>Otherwise, you'll be using a CPU for the following computations. This
is fine for our small dataset but it will take longer for larger
datasets.</p>
<h3 id="data">6.1 Data</h3>
<p>Let's create some data just like before.</p>
<p>First, we'll hard-code some <code>weight</code> and <code>bias</code>
values.</p>
<p>Then we'll make a range of numbers between 0 and 1, these will be our
<code>X</code> values.</p>
<p>Finally, we'll use the <code>X</code> values, as well as the
<code>weight</code> and <code>bias</code> values to create
<code>y</code> using the linear regression formula
(<code>y = weight * X + bias</code>).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create weight and bias</span></span><br><span class="line">weight = <span class="number">0.7</span></span><br><span class="line">bias = <span class="number">0.3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create range values</span></span><br><span class="line">start = <span class="number">0</span></span><br><span class="line">end = <span class="number">1</span></span><br><span class="line">step = <span class="number">0.02</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create X and y (features and labels)</span></span><br><span class="line">X = torch.arange(start, end, step).unsqueeze(dim=<span class="number">1</span>) <span class="comment"># without unsqueeze, errors will happen later on (shapes within linear layers)</span></span><br><span class="line">y = weight * X + bias </span><br><span class="line">X[:<span class="number">10</span>], y[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>
<pre><code>(tensor([[0.0000],
         [0.0200],
         [0.0400],
         [0.0600],
         [0.0800],
         [0.1000],
         [0.1200],
         [0.1400],
         [0.1600],
         [0.1800]]),
 tensor([[0.3000],
         [0.3140],
         [0.3280],
         [0.3420],
         [0.3560],
         [0.3700],
         [0.3840],
         [0.3980],
         [0.4120],
         [0.4260]]))</code></pre>
<p>Wonderful!</p>
<p>Now we've got some data, let's split it into training and test
sets.</p>
<p>We'll use an 80/20 split with 80% training data and 20% testing
data.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Split data</span></span><br><span class="line">train_split = <span class="built_in">int</span>(<span class="number">0.8</span> * <span class="built_in">len</span>(X))</span><br><span class="line">X_train, y_train = X[:train_split], y[:train_split]</span><br><span class="line">X_test, y_test = X[train_split:], y[train_split:]</span><br><span class="line"></span><br><span class="line"><span class="built_in">len</span>(X_train), <span class="built_in">len</span>(y_train), <span class="built_in">len</span>(X_test), <span class="built_in">len</span>(y_test)</span><br></pre></td></tr></table></figure>
<pre><code>(40, 40, 10, 10)</code></pre>
<p>Excellent, let's visualize them to make sure they look okay.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Note: If you&#x27;ve reset your runtime, this function won&#x27;t work, </span></span><br><span class="line"><span class="comment"># you&#x27;ll have to rerun the cell above where it&#x27;s instantiated.</span></span><br><span class="line">plot_predictions(X_train, y_train, X_test, y_test)</span><br></pre></td></tr></table></figure>
<p>​<br />
<img src="/image/pytorch-learn-01/output_70_0.png" alt="png" /> ​</p>
<h3 id="building-a-pytorch-linear-model">6.2 Building a PyTorch linear
model</h3>
<p>We've got some data, now it's time to make a model.</p>
<p>We'll create the same style of model as before except this time,
instead of defining the weight and bias parameters of our model manually
using <code>nn.Parameter()</code>, we'll use <a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html"><code>nn.Linear(in_features, out_features)</code></a>
to do it for us.</p>
<p>Where <code>in_features</code> is the number of dimensions your input
data has and <code>out_features</code> is the number of dimensions you'd
like it to be output to.</p>
<p>In our case, both of these are <code>1</code> since our data has
<code>1</code> input feature (<code>X</code>) per label
(<code>y</code>).</p>
<p><img
src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01-pytorch-linear-regression-model-with-nn-Parameter-and-nn-Linear-compared.png"
alt="comparison of nn.Parameter Linear Regression model and nn.Linear Linear Regression model" />
<em>Creating a linear regression model using <code>nn.Parameter</code>
versus using <code>nn.Linear</code>. There are plenty more examples of
where the <code>torch.nn</code> module has pre-built computations,
including many popular and useful neural network layers.</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Subclass nn.Module to make our model</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearRegressionModelV2</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># Use nn.Linear() for creating the model parameters</span></span><br><span class="line">        self.linear_layer = nn.Linear(in_features=<span class="number">1</span>, </span><br><span class="line">                                      out_features=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define the forward computation (input data x flows through nn.Linear())</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        <span class="keyword">return</span> self.linear_layer(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the manual seed when creating the model (this isn&#x27;t always need but is used for demonstrative purposes, try commenting it out and seeing what happens)</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line">model_1 = LinearRegressionModelV2()</span><br><span class="line">model_1, model_1.state_dict()</span><br></pre></td></tr></table></figure>
<pre><code>(LinearRegressionModelV2(
   (linear_layer): Linear(in_features=1, out_features=1, bias=True)
 ),
 OrderedDict([(&#39;linear_layer.weight&#39;, tensor([[0.7645]])),
              (&#39;linear_layer.bias&#39;, tensor([0.8300]))]))</code></pre>
<p>Notice the outputs of <code>model_1.state_dict()</code>, the
<code>nn.Linear()</code> layer created a random <code>weight</code> and
<code>bias</code> parameter for us.</p>
<p>Now let's put our model on the GPU (if it's available).</p>
<p>We can change the device our PyTorch objects are on using
<code>.to(device)</code>.</p>
<p>First let's check the model's current device.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Check model device</span></span><br><span class="line"><span class="built_in">next</span>(model_1.parameters()).device</span><br></pre></td></tr></table></figure>
<pre><code>device(type=&#39;cpu&#39;)</code></pre>
<p>Wonderful, looks like the model's on the CPU by default.</p>
<p>Let's change it to be on the GPU (if it's available).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set model to GPU if it&#x27;s availalble, otherwise it&#x27;ll default to CPU</span></span><br><span class="line">model_1.to(device) <span class="comment"># the device variable was set above to be &quot;cuda&quot; if available or &quot;cpu&quot; if not</span></span><br><span class="line"><span class="built_in">next</span>(model_1.parameters()).device</span><br></pre></td></tr></table></figure>
<pre><code>device(type=&#39;cuda&#39;, index=0)</code></pre>
<p>Nice! Because of our device agnostic code, the above cell will work
regardless of whether a GPU is available or not.</p>
<p>If you do have access to a CUDA-enabled GPU, you should see an output
of something like:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">device(type=&#x27;cuda&#x27;, index=0)</span><br></pre></td></tr></table></figure>
<h3 id="training">6.3 Training</h3>
<p>Time to build a training and testing loop.</p>
<p>First we'll need a loss function and an optimizer.</p>
<p>Let's use the same functions we used earlier,
<code>nn.L1Loss()</code> and <code>torch.optim.SGD()</code>.</p>
<p>We'll have to pass the new model's parameters
(<code>model.parameters()</code>) to the optimizer for it to adjust them
during training.</p>
<p>The learning rate of <code>0.01</code> worked well before too so
let's use that again.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create loss function</span></span><br><span class="line">loss_fn = nn.L1Loss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create optimizer</span></span><br><span class="line">optimizer = torch.optim.SGD(params=model_1.parameters(), <span class="comment"># optimize newly created model&#x27;s parameters</span></span><br><span class="line">                            lr=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>
<p>Beautiful, loss function and optimizer ready, now let's train and
evaluate our model using a training and testing loop.</p>
<p>The only different thing we'll be doing in this step compared to the
previous training loop is putting the data on the target
<code>device</code>.</p>
<p>We've already put our model on the target <code>device</code> using
<code>model_1.to(device)</code>.</p>
<p>And we can do the same with the data.</p>
<p>That way if the model is on the GPU, the data is on the GPU (and vice
versa).</p>
<p>Let's step things up a notch this time and set
<code>epochs=1000</code>.</p>
<p>If you need a reminder of the PyTorch training loop steps, see
below.</p>
<details>
<summary>
PyTorch training loop steps
</summary>
<ol>
<li>
<b>Forward pass</b> - The model goes through all of the training data
once, performing its <code>forward()</code> function calculations
(<code>model(x_train)</code>).
</li>
<li>
<b>Calculate the loss</b> - The model's outputs (predictions) are
compared to the ground truth and evaluated to see how wrong they are
(<code>loss = loss_fn(y_pred, y_train</code>).
</li>
<li>
<b>Zero gradients</b> - The optimizers gradients are set to zero (they
are accumulated by default) so they can be recalculated for the specific
training step (<code>optimizer.zero_grad()</code>).
</li>
<li>
<b>Perform backpropagation on the loss</b> - Computes the gradient of
the loss with respect for every model parameter to be updated (each
parameter with <code>requires_grad=True</code>). This is known as
<b>backpropagation</b>, hence "backwards"
(<code>loss.backward()</code>).
</li>
<li>
<b>Step the optimizer (gradient descent)</b> - Update the parameters
with <code>requires_grad=True</code> with respect to the loss gradients
in order to improve them (<code>optimizer.step()</code>).
</li>
</ol>
</details>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the number of epochs </span></span><br><span class="line">epochs = <span class="number">1000</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># Put data on the available device</span></span><br><span class="line"><span class="comment"># Without this, error will happen (not all model/data on device)</span></span><br><span class="line">X_train = X_train.to(device)</span><br><span class="line">X_test = X_test.to(device)</span><br><span class="line">y_train = y_train.to(device)</span><br><span class="line">y_test = y_test.to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment">### Training</span></span><br><span class="line">    model_1.train() <span class="comment"># train mode is on by default after construction</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. Forward pass</span></span><br><span class="line">    y_pred = model_1(X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. Calculate loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. Zero grad optimizer</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. Loss backward</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. Step the optimizer</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment">### Testing</span></span><br><span class="line">    model_1.<span class="built_in">eval</span>() <span class="comment"># put the model in evaluation mode for testing (inference)</span></span><br><span class="line">    <span class="comment"># 1. Forward pass</span></span><br><span class="line">    <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">        test_pred = model_1(X_test)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 2. Calculate the loss</span></span><br><span class="line">        test_loss = loss_fn(test_pred, y_test)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch&#125;</span> | Train loss: <span class="subst">&#123;loss&#125;</span> | Test loss: <span class="subst">&#123;test_loss&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch: 0 | Train loss: 0.5551779866218567 | Test loss: 0.5739762187004089
Epoch: 100 | Train loss: 0.006215683650225401 | Test loss: 0.014086711220443249
Epoch: 200 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882
Epoch: 300 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882
Epoch: 400 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882
Epoch: 500 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882
Epoch: 600 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882
Epoch: 700 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882
Epoch: 800 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882
Epoch: 900 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882</code></pre>
<blockquote>
<p><strong>Note:</strong> Due to the random nature of machine learning,
you will likely get slightly different results (different loss and
prediction values) depending on whether your model was trained on CPU or
GPU. This is true even if you use the same random seed on either device.
If the difference is large, you may want to look for errors, however, if
it is small (ideally it is), you can ignore it.</p>
</blockquote>
<p>Nice! That loss looks pretty low.</p>
<p>Let's check the parameters our model has learned and compare them to
the original parameters we hard-coded.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Find our model&#x27;s learned parameters</span></span><br><span class="line"><span class="keyword">from</span> pprint <span class="keyword">import</span> pprint <span class="comment"># pprint = pretty print, see: https://docs.python.org/3/library/pprint.html </span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The model learned the following values for weights and bias:&quot;</span>)</span><br><span class="line">pprint(model_1.state_dict())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nAnd the original values for weights and bias are:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;weights: <span class="subst">&#123;weight&#125;</span>, bias: <span class="subst">&#123;bias&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>The model learned the following values for weights and bias:
OrderedDict([(&#39;linear_layer.weight&#39;, tensor([[0.6968]], device=&#39;cuda:0&#39;)),
             (&#39;linear_layer.bias&#39;, tensor([0.3025], device=&#39;cuda:0&#39;))])

And the original values for weights and bias are:
weights: 0.7, bias: 0.3</code></pre>
<p>Ho ho! Now that's pretty darn close to a perfect model.</p>
<p>Remember though, in practice, it's rare that you'll know the perfect
parameters ahead of time.</p>
<p>And if you knew the parameters your model had to learn ahead of time,
what would be the fun of machine learning?</p>
<p>Plus, in many real-world machine learning problems, the number of
parameters can well exceed tens of millions.</p>
<p>I don't know about you but I'd rather write code for a computer to
figure those out rather than doing it by hand.</p>
<h3 id="making-predictions">6.4 Making predictions</h3>
<p>Now we've got a trained model, let's turn on it's evaluation mode and
make some predictions.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Turn model into evaluation mode</span></span><br><span class="line">model_1.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make predictions on the test data</span></span><br><span class="line"><span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">    y_preds = model_1(X_test)</span><br><span class="line">y_preds</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[0.8600],
        [0.8739],
        [0.8878],
        [0.9018],
        [0.9157],
        [0.9296],
        [0.9436],
        [0.9575],
        [0.9714],
        [0.9854]], device=&#39;cuda:0&#39;)</code></pre>
<p>If you're making predictions with data on the GPU, you might notice
the output of the above has <code>device='cuda:0'</code> towards the
end. That means the data is on CUDA device 0 (the first GPU your system
has access to due to zero-indexing), if you end up using multiple GPUs
in the future, this number may be higher.</p>
<p>Now let's plot our model's predictions.</p>
<blockquote>
<p><strong>Note:</strong> Many data science libraries such as pandas,
matplotlib and NumPy aren't capable of using data that is stored on GPU.
So you might run into some issues when trying to use a function from one
of these libraries with tensor data not stored on the CPU. To fix this,
you can call <a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cpu.html"><code>.cpu()</code></a>
on your target tensor to return a copy of your target tensor on the
CPU.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plot_predictions(predictions=y_preds) # -&gt; won&#x27;t work... data not on CPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Put data on the CPU and plot it</span></span><br><span class="line">plot_predictions(predictions=y_preds.cpu())</span><br></pre></td></tr></table></figure>
<p>​<br />
<img src="/image/pytorch-learn-01/output_89_0.png" alt="png" /> ​</p>
<p>Woah! Look at those red dots, they line up almost perfectly with the
green dots. I guess the extra epochs helped.</p>
<h3 id="saving-and-loading-a-model">6.5 Saving and loading a model</h3>
<p>We're happy with our models predictions, so let's save it to file so
it can be used later.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Create models directory </span></span><br><span class="line">MODEL_PATH = Path(<span class="string">&quot;models&quot;</span>)</span><br><span class="line">MODEL_PATH.mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Create model save path </span></span><br><span class="line">MODEL_NAME = <span class="string">&quot;01_pytorch_workflow_model_1.pth&quot;</span></span><br><span class="line">MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Save the model state dict </span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Saving model to: <span class="subst">&#123;MODEL_SAVE_PATH&#125;</span>&quot;</span>)</span><br><span class="line">torch.save(obj=model_1.state_dict(), <span class="comment"># only saving the state_dict() only saves the models learned parameters</span></span><br><span class="line">           f=MODEL_SAVE_PATH) </span><br></pre></td></tr></table></figure>
<pre><code>Saving model to: models/01_pytorch_workflow_model_1.pth</code></pre>
<p>And just to make sure everything worked well, let's load it back
in.</p>
<p>We'll: * Create a new instance of the
<code>LinearRegressionModelV2()</code> class * Load in the model state
dict using <code>torch.nn.Module.load_state_dict()</code> * Send the new
instance of the model to the target device (to ensure our code is
device-agnostic)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Instantiate a fresh instance of LinearRegressionModelV2</span></span><br><span class="line">loaded_model_1 = LinearRegressionModelV2()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load model state dict </span></span><br><span class="line">loaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Put model to target device (if your data is on GPU, model will have to be on GPU to make predictions)</span></span><br><span class="line">loaded_model_1.to(device)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Loaded model:\n<span class="subst">&#123;loaded_model_1&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Model on device:\n<span class="subst">&#123;<span class="built_in">next</span>(loaded_model_1.parameters()).device&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Loaded model:
LinearRegressionModelV2(
  (linear_layer): Linear(in_features=1, out_features=1, bias=True)
)
Model on device:
cuda:0</code></pre>
<p>Now we can evaluate the loaded model to see if its predictions line
up with the predictions made prior to saving.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Evaluate loaded model</span></span><br><span class="line">loaded_model_1.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">    loaded_model_1_preds = loaded_model_1(X_test)</span><br><span class="line">y_preds == loaded_model_1_preds</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[True],
        [True],
        [True],
        [True],
        [True],
        [True],
        [True],
        [True],
        [True],
        [True]], device=&#39;cuda:0&#39;)</code></pre>
<p>Everything adds up! Nice!</p>
<p>Well, we've come a long way. You've now built and trained your first
two neural network models in PyTorch!</p>
<p>Time to practice your skills.</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="https://github.com/Confetti-lxy">Confetti-lxy</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://blog.confetti-lxy.com/2023/05/12/pytorch-learn-01/">http://blog.confetti-lxy.com/2023/05/12/pytorch-learn-01/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/torch/">torch</a></div><div class="post_share"><div class="social-share" data-image="/img/cover/cover-01.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/05/14/decision-tree/" title="decision-tree"><img class="cover" src="/img/cover/cover-01.png" onerror="onerror=null;src='/img/common/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">decision-tree</div></div></a></div><div class="next-post pull-right"><a href="/2023/05/12/pytorch-learn-00/" title="pytorch-learn-00"><img class="cover" src="/img/cover/cover-12.jpeg" onerror="onerror=null;src='/img/common/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">pytorch-learn-00</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2023/05/12/pytorch-learn-00/" title="pytorch-learn-00"><img class="cover" src="/img/cover/cover-12.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-12</div><div class="title">pytorch-learn-00</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/common/avatar.png" onerror="this.onerror=null;this.src='/img/common/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Confetti-lxy</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">10</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Confetti-lxy"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Confetti-lxy" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:3038454387@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">总要记些什么证明我来过</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#pytorch-workflow-fundamentals"><span class="toc-number">1.</span> <span class="toc-text">01. PyTorch Workflow
Fundamentals</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#what-were-going-to-cover"><span class="toc-number">1.1.</span> <span class="toc-text">What we&#39;re going to cover</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#where-can-can-you-get-help"><span class="toc-number">1.2.</span> <span class="toc-text">Where can can you get help?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#data-preparing-and-loading"><span class="toc-number">1.3.</span> <span class="toc-text">1. Data (preparing and loading)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#split-data-into-training-and-test-sets"><span class="toc-number">1.3.1.</span> <span class="toc-text">Split data into training
and test sets</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#build-model"><span class="toc-number">1.4.</span> <span class="toc-text">2. Build model</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#pytorch-model-building-essentials"><span class="toc-number">1.4.1.</span> <span class="toc-text">PyTorch model building
essentials</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#checking-the-contents-of-a-pytorch-model"><span class="toc-number">1.4.2.</span> <span class="toc-text">Checking the contents
of a PyTorch model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#making-predictions-using-torch.inference_mode"><span class="toc-number">1.4.3.</span> <span class="toc-text">Making
predictions using torch.inference_mode()</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#train-model"><span class="toc-number">1.5.</span> <span class="toc-text">3. Train model</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#creating-a-loss-function-and-optimizer-in-pytorch"><span class="toc-number">1.5.1.</span> <span class="toc-text">Creating a
loss function and optimizer in PyTorch</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#creating-an-optimization-loop-in-pytorch"><span class="toc-number">1.5.2.</span> <span class="toc-text">Creating an
optimization loop in PyTorch</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pytorch-training-loop"><span class="toc-number">1.5.3.</span> <span class="toc-text">PyTorch training loop</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pytorch-testing-loop"><span class="toc-number">1.5.4.</span> <span class="toc-text">PyTorch testing loop</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#making-predictions-with-a-trained-pytorch-model-inference"><span class="toc-number">1.6.</span> <span class="toc-text">4.
Making predictions with a trained PyTorch model (inference)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#saving-and-loading-a-pytorch-model"><span class="toc-number">1.7.</span> <span class="toc-text">5. Saving and loading a
PyTorch model</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#saving-a-pytorch-models-state_dict"><span class="toc-number">1.7.1.</span> <span class="toc-text">Saving a PyTorch model&#39;s
state_dict()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#loading-a-saved-pytorch-models-state_dict"><span class="toc-number">1.7.2.</span> <span class="toc-text">Loading a saved
PyTorch model&#39;s state_dict()</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#putting-it-all-together"><span class="toc-number">1.8.</span> <span class="toc-text">6. Putting it all together</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#data"><span class="toc-number">1.8.1.</span> <span class="toc-text">6.1 Data</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#building-a-pytorch-linear-model"><span class="toc-number">1.8.2.</span> <span class="toc-text">6.2 Building a PyTorch linear
model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#training"><span class="toc-number">1.8.3.</span> <span class="toc-text">6.3 Training</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#making-predictions"><span class="toc-number">1.8.4.</span> <span class="toc-text">6.4 Making predictions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#saving-and-loading-a-model"><span class="toc-number">1.8.5.</span> <span class="toc-text">6.5 Saving and loading a model</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/05/14/Home-Credit-Default-Risk/" title="Home-Credit-Default-Risk"><img src="/img/cover/cover-14.jpeg" onerror="this.onerror=null;this.src='/img/common/404.jpg'" alt="Home-Credit-Default-Risk"/></a><div class="content"><a class="title" href="/2023/05/14/Home-Credit-Default-Risk/" title="Home-Credit-Default-Risk">Home-Credit-Default-Risk</a><time datetime="2023-05-14T10:36:42.000Z" title="Created 2023-05-14 18:36:42">2023-05-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/14/decision-tree/" title="decision-tree"><img src="/img/cover/cover-01.png" onerror="this.onerror=null;this.src='/img/common/404.jpg'" alt="decision-tree"/></a><div class="content"><a class="title" href="/2023/05/14/decision-tree/" title="decision-tree">decision-tree</a><time datetime="2023-05-14T05:55:12.000Z" title="Created 2023-05-14 13:55:12">2023-05-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/12/pytorch-learn-01/" title="pytorch-learn-01"><img src="/img/cover/cover-01.png" onerror="this.onerror=null;this.src='/img/common/404.jpg'" alt="pytorch-learn-01"/></a><div class="content"><a class="title" href="/2023/05/12/pytorch-learn-01/" title="pytorch-learn-01">pytorch-learn-01</a><time datetime="2023-05-12T15:25:52.000Z" title="Created 2023-05-12 23:25:52">2023-05-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/12/pytorch-learn-00/" title="pytorch-learn-00"><img src="/img/cover/cover-12.jpeg" onerror="this.onerror=null;this.src='/img/common/404.jpg'" alt="pytorch-learn-00"/></a><div class="content"><a class="title" href="/2023/05/12/pytorch-learn-00/" title="pytorch-learn-00">pytorch-learn-00</a><time datetime="2023-05-12T15:10:52.000Z" title="Created 2023-05-12 23:10:52">2023-05-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/04/29/plcs/" title="plcs"><img src="/img/cover/cover-12.jpeg" onerror="this.onerror=null;this.src='/img/common/404.jpg'" alt="plcs"/></a><div class="content"><a class="title" href="/2023/04/29/plcs/" title="plcs">plcs</a><time datetime="2023-04-29T15:53:13.000Z" title="Created 2023-04-29 23:53:13">2023-04-29</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/PL/PL-2.jpeg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Confetti-lxy</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">世界很大,我想去看看</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Toggle Between Traditional Chinese And Simplified Chinese">繁</button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="true" data-click="true"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>